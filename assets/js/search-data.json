{
  
    
        "post0": {
            "title": "UMAP for Flow Cytometry",
            "content": "Flow cytometry is a powerful technique for phenotypic analysis of cells and cell populations. One main challenge in flow cytometry analysis is to visualise the resulting high-dimensional data to understand data at single-cell levels. . This is where dimensionality reduction techniques come at play, in particular with the advent of spectral and mass cytometry. Most biological data are non-linear so using algorithms based on principal component (the famous PCA) will lose local distributions even if they maintain global ones. The most used embedding algorithm (i.e. that transform high-diemnsion in low-dimension) is currently t-SNE, which stands for t-Stochastic Neighbor Embedding. I will tell you here why UMAP killed t-SNE as nicely said in this article. . A little bit about t-SNE before killing it . Let’s start by explaining briefly how t-SNE works: Developped by L.J.P. van der Maaten, the algorithm aims is to minimise the Kullback-Leibler (KL) divergence between the low-dimensional embedding and the high-dimensional data. The KL divergence can be defined as a measure of how one probability distribution diverges from a second. （；¬＿¬) . Ok, let&#39;s plot KL divergence to better understand it using scipy.stats norm and numpy for distribution and seaborn and matplotlib.pyplot for plotting . import pandas as pd import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm . Let&#39;s create a little function that plots two normal distribution and calculate the KL . def kl_divergence(x): p = norm.pdf(x, 0, 2) q = norm.pdf(x, 3, 2) kl = p * np.log(p /q) return p, q, kl . x = np.arange(-10, 10, 0.1) p, q, kl = kl_divergence(x) . sns.set(&#39;paper&#39;, &#39;whitegrid&#39;) plt.plot(x, p, c=&#39;b&#39;) plt.plot(x, q, c=&#39;r&#39;) plt.plot(x, kl, lw=0.1, c=&#39;b&#39;) plt.fill_between(x, kl, alpha=0.2, color=&#39;b&#39;) plt.title(&#39;KL(P||Q)&#39;) plt.ylim(-0.1, 0.35) plt.show() . So the idea if to arrive to: . Steps . In high dimension, t-SNE tries to determine the probability of similarity between each data points. To do so, t-SNE: | measures the (‘Euclidean’, sort of straight) distance between one point to all the other points, pairwise. | computes a similarity score using a Gaussian distribution kernel: the probability a point x would have another point as neighbour if this neighbour was picked from a normal distribution centred on the point x. | These similarity scores are then normalised to account for cluster density. The expected density around each point that determines the width of the kernel is controlled by the perplexity parameters. | To account for unbalanced density and thus disagreeing scores between two points, the similarity are symmetrised by averaging them. This gives you a matrix of similarity scores for the high-dimensional data. | . Following dimension reduction initialisation with PCA or at random, the data are embedded in low dimension. The process describe above is performed again but this time to avoid overcrowding clusters (all data points ending up in the same area) a t-distribution and not a Gaussian is used leaving heavy tails for far apart values. | The KL divergence cost function is minimised by gradient descent, i.e. by small steps. The use of KL diverge prevent t-SNE from conserving global distance in low-dimension. | . Parameters . t-SNE has two important hyperparameters to tune: . perplexity: number of nearest neighbours whose geometric distance is preserved in embedding, i.e. that t-SNE is considering. Typical value range from 5-50, but the larger the dataset the larger the perplexity. | max. number of iteration: ideally the more the better, but it is limited by computational time. A default value of 1000 is good for exploration. | . . Note: t-SNE has a cost function that is not convex, i.e. it can be stuck at a local minima and with different initialisation we can get different results. . Pros and cons . Advantages: . preserves local structure and clusters | . Disadvantages: . loss of global structure | slow computation | suffers from overcrowding in large datasets | . To go further . For simple and visual explanation, I would recommend StatQuest from Josh Starmer and this popular tutorial . UMAP . Uniform Manifold Approximation and Projection or UMAP was developed in 2018 by McInnes. UMAP has already been integrated into proprietary software for flow analysis democratising its use but its python library offers a wider range of possibility. . Steps . UMAP builts a graph of the high dimensional data | As t-SNE, UMAP relies on building a graph of the high-dimensional data, i.e. a network of nodes connected by edges. The edge are weighted by the probability of the two points to be connected. The connection between to points is made when the radius around one point overlap the radius of another points. . How to choose the radius? UMAP chooses a radius for each points using the distance between the point and its k-th nearest neighbour. Then it computes the likelihood of connection as inversely proportional to the radius growth, making the graph ‘fuzzy’. This last point is to solve the curse of dimensionality where the distances between points become more similar with higher dimensions. . To avoid ending up with isolated points and maintaining local structure, UMAP ensures that each point is connected to a least its closest neighbour. As in t-SNE, UMAP can generate connections with incompatible weights, however instead of just averaging the weighs UMAP uses the union of the two ‘fuzzy simplical sets’. . . Example of fuzzy radii and weighted connections adapted from the [page](https://pair-code.github.io/understanding-umap/) of A.Coenen and A.Pearce . UMAP optimises the low dimension | As in t-SNE, the point is to optimise the low dimension data (that do not have varying metrics like the high dimension above but Euclidean distances) so they are the closest to the ‘fuzzy’ topology define above. However, to do so instead of using KL divergence, UMAP uses cross-entropy as a loss of function. This function is not too far from KL divergence, but, by using attractive and repulsive forces, it maintains global structure better. Moreover, unlike t-SNE, UMAP does not use random initialisation for embedding but spectral embedding so UMAP is more reproducible from run to run. . Note: it is still stochastic so it does change a bit. . Finally, UMAP uses stochastic gradient descent instead of gradient descent so it is computed faster. . Parameters . n_neighbors: number of neighbours for the high dimensional graph. A low value will provides with local structure while a high value will focus on global structure and lose fine details. | min_dist: minimum distance between points in low dimension,i.e. how tight the points are clustered. Large values results in less pack clusters and loses focus on global structure. | . Pros and cons . Advantages: . preserves local structure and clusters | better preservation of global structure | better reproducibility | fast runtime | . Disadvantages: . hyperparameter choice is crucial and depends on the aims | . To go further . For a really playful and clear detailed explanation, checkout this website. . UMAP for flow cytometry data . UMAP can be used for flow analysis with several aims as described in details in the doc from UMAP, e.g. to plot high-dimensional data or to reduce dimension before clustering. Here, we will explore how UMAP can help in the visualisation of cardiac immune cell response during chronic kidney disease by looking at changes in cardiac leukocyte subtypes. . Getting the data ready for it . . From FlowJo to gated data . A real cool tool available for python is Flowkit. You can use this tool to extract data that have been gated with GatingML 2.0 or FlowJo 10. I willl focus here on FlowJo workspace as this is were I did my gating. . To work with FlowJo workspace you need to first call a Session to get your FCS and import your workspace in it. Then you need to apply the gating to the sample group (the group from your FlowJo workspace structure). . fks_fj = fk.Session(&#39;../data/raw_data&#39;) fks_fj.import_flowjo_workspace(&#39;../data/raw_data/FJ_workspace.wsp&#39;) analysis = fks_fj.analyze_samples(&#39;group&#39;) . To get your labelled gated sampled all you need to do is: . 1.Get a dataframe of the gating raw results where in the column events you will have an array of booleans for each gates . gates = pd.DataFrame(fks_fj.get_gating_results(&#39;group&#39;, &#39;sample&#39;)._raw_results).loc[&#39;events&#39;] #where sample is your sample&#39;s name ◔‿◔ gates = pd.DataFrame(gates.droplevel(1)).transpose() . 2.Explode the arrays into single rows . gated = pd.DataFrame() for col in gates.columns: gated[col] = gates[col].explode(ignore_index=True) . 3.Merge it back with the fluorescence values of each single cell . Here you can also select only a subpopulation if you know what is your population of interest (for example not the singlets) . gated.merge(fkj.get_sample(&#39;sample&#39;).as_dataframe(&#39;raw&#39;), left_index=True, right_index=True) . . Tip: fks_fj.get_gate_hierarchy is a useful call to check your gating strategy if you have forgotten it . Now that you have extracted your data, there is just one last crucial step. . Preprocessing: arcsinh . Data are transformed to using arcsinh with a cofactor of 150, so it is close to the classic biex but has a linear representation. Low-end values are compressed close to zero (more or less depending on the cofactor) . umap_data = np.arcsinh(features.values / 150.0).astype(np.float32, order=&#39;C&#39;) . sns.set(font_scale=1.3, style=&#39;ticks&#39;, context=&#39;talk&#39;, font=&#39;Arial&#39;, rc={&quot;axes.linewidth&quot;: 3, &quot;xtick.major.size&quot;: 4, &quot;ytick.major.size&quot;: 4, &#39;legend.handletextpad&#39;: 0.05, &#39;legend.markerscale&#39;: 1.3, &#39;axes.labelpad&#39;: 2.0, &#39;legend.columnspacing&#39;: 0.8, }) g = sns.relplot(x=&#39;UMAP 1&#39;, y=&#39;UMAP 2&#39;, data=graph, hue=&#39;subtype&#39;, col=&#39;sample&#39;, col_order=[&#39;sham&#39;, &#39;nx&#39;], s=4, alpha=0.3, marker=&#39;o&#39;, linewidth=0, palette=sns.xkcd_palette([&#39;cherry&#39;, &#39;nice blue&#39;, &#39;eggplant&#39;, &#39;powder pink&#39;, &#39;green apple&#39;, &#39;medium green&#39;, &#39;apricot&#39;, &#39;light grey&#39;]), facet_kws={&#39;sharey&#39;: False, &#39;legend_out&#39;: True}, ) for ax, title in zip(g.axes.flat, [&#39;Sham&#39;, &#39;5/6 Nx&#39;]): ax.set_title(title, fontsize=22, fontweight=&#39;semibold&#39;) g._legend.set_title(&#39;&#39;) g.set_xticklabels(fontsize=13) g.set_yticklabels(fontsize=13) g.set(ylim=(-8, 10)) plt.subplots_adjust(wspace=0.16) g._legend.remove() h, l = g.axes[0,0].get_legend_handles_labels() plt.legend(h, l , ncol=3, bbox_to_anchor=(1.05, -0.2), frameon=False) . &lt;matplotlib.legend.Legend at 0x7fca45a17dc0&gt; .",
            "url": "https://marie-annemawhin.github.io/blog/2021/01/29/parametric_UMAP_blog.html",
            "relUrl": "/2021/01/29/parametric_UMAP_blog.html",
            "date": " • Jan 29, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "A little about me",
          "content": "I’m a biologist and neophyte data scientist. I have been exploring the world of machine learning and deep learning and I want to share my understanding of these models with you. I hope I can provide you with an accurate and simple insight into AI. . You can check out my CV and find out more about what I’m up to right now here. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://marie-annemawhin.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://marie-annemawhin.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}