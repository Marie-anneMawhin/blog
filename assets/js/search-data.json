{
  
    
        "post0": {
            "title": "UMAP for Flow Cytometry - Part 2",
            "content": "UMAP for flow cytometry data . UMAP can be used for flow analysis with several aims as described in details in the doc from UMAP, e.g. to plot high-dimensional data or to reduce dimension before clustering. Here, we will explore how UMAP can help in the visualisation of cardiac immune cell response during chronic kidney disease by looking at changes in cardiac leukocyte subtypes. . Getting the data ready for it . . From FlowJo to gated data . A real cool tool available for python is Flowkit. You can use this tool to extract data that have been gated with GatingML 2.0 or FlowJo 10. I willl focus here on a FlowJo workspace as this is were I did my gating. . To work with FlowJo workspace you need to first call a Session to get your FCS and import your workspace in it. Then you need to apply the gating to the sample group (the group from your FlowJo workspace structure). . fks_fj = fk.Session(&#39;../data/raw_data&#39;) fks_fj.import_flowjo_workspace(&#39;../data/raw_data/FJ_workspace.wsp&#39;) analysis = fks_fj.analyze_samples(&#39;group&#39;) . To get your labelled gated sampled all you need to do is: . 1.Get a dataframe of the gating raw results where in the column events you will have an array of booleans for each gates . gates = pd.DataFrame(fks_fj.get_gating_results(&#39;group&#39;, &#39;sample&#39;)._raw_results).loc[&#39;events&#39;] #where sample is your sample&#39;s name ◔‿◔ gates = pd.DataFrame(gates.droplevel(1)).transpose() . 2.Explode the arrays into single rows . gated = pd.DataFrame() for col in gates.columns: gated[col] = gates[col].explode(ignore_index=True) . 3.Merge it back with the fluorescence values of each single cell . Here you can also select only a subpopulation if you know what is your population of interest (for example not the singlets) . features = gated.merge(fkj.get_sample(&#39;sample&#39;).as_dataframe(&#39;raw&#39;), left_index=True, right_index=True) . . Tip: fks_fj.get_gate_hierarchy is a useful call to check your gating strategy if you have forgotten it . Now that you have extracted your data, there is just one last crucial step. . Preprocessing: arcsinh . Data from high intensity values are spread on a log scale but the problem with log-scale is that it does not do well on the &#39;low-end&#39; of the intensity scale, especially since some fluorescence values are zero or sometimes negative (following baseline correction). To solve this problem, bi-exponential transformation have been applied to flow data to approach log for high values and linearity for data around zero. The ArcSinh transform is close to the classic bi-exponential, with a linear representation near zero and a log-like scale beyond a certain threshold, but it is equation-based. The threshold or co-factor determines the compression of the low-end values, the higher the more data are compressed close to zero. Here, data were transformed using arcsinh with a co-factor of 150 as suggested by Bendall. . . Arcsinh transformation adapted from [Bendall et al.](https://science.sciencemag.org/content/332/6030/687) . umap_data = np.arcsinh(features.values / 150.0).astype(np.float32, order=&#39;C&#39;) . . Note: It is good practice to validate the cofactor value and the transformation by visualising each channel to avoid under/over compression. . Our data . The data use in this case study come from a flow analysis of immune cells in healthy heart vs chronic kidney disease heart. The panel uses 10 antibodies to identify 7 populations of immune cells. . Adapted from Epelman et al. 2014 . Non-parametric UMAP . Let&#39;s explore what UMAP do. The library provides a really detailed documentation and also nice functions to explore UMAP. . UMAP is used in the style of most sklearn models: instantiate and fit or fit/transform. . The library also offers utility functions: . plot.points: plot a scatter plot. plot.connectivity: to plot the connection between each points. plot.diagnostic: to diagnose the embedding. . Reminder of parameters:n_neighbors: number of neighbours for the high dimensional graph. min_dist: minimum distance between points in low dimension,i.e. how tight the points are clustered. . import umap.plot import umap #common usage mapper = umap.UMAP() embedding = mapper.fit(umap_data) . Neighbours and minimum distance . Let&#39;s test what happen when we modify the hyperparameters: . def draw_umap(n_neighbors=15, min_dist=0.1, title=&#39;&#39;): mapper = umap.UMAP( n_neighbors=n_neighbors, min_dist=min_dist, random_state=42 ).fit(umap_data) umap.plot.points(mapper, labels=labels, theme=&#39;blue&#39;) plt.title(title, fontsize=15) plt.show() . On the following plots, the colours represent gated cells. You can truly see that the identified populations are nicely separated by UMAP. . Conclusion . A low number of neighbours conserves local structure while a high value focuses more on global structure and loses fine details. | Larger distance results in less packed clusters and loses focus on global structure. | . The point is to find an ideal balance for your problematic. . Diagnostic and connectivity . umap.plot.connectivity(mapper, show_points=True, width=400, height=400) plt.show() . umap.plot.diagnostic(mapper, diagnostic_type=&#39;pca&#39;, width=400, height=400) plt.show() . Parametric UMAP . Parametric UMAP is really powerful as it relies on a neural network to tune the hyperparameters. You can tune the network if needed but I found that it performed really well out of the box. . Loss is binary cross-entropy so we need to scale the data in range 0-1 with min max scaler: . from sklearn.preprocessing import MinMaxScaler scale = MinMaxScaler() scaled_umap_data = scale.fit_transform(umap_data) embedder = ParametricUMAP() embedding = embedder.fit_transform(scaled_umap_data) embedder.encoder.summary() . Model: &#34;sequential_11&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten_11 (Flatten) (None, 10) 0 _________________________________________________________________ dense_33 (Dense) (None, 100) 1100 _________________________________________________________________ dense_34 (Dense) (None, 100) 10100 _________________________________________________________________ dense_35 (Dense) (None, 100) 10100 _________________________________________________________________ z (Dense) (None, 2) 202 ================================================================= Total params: 21,502 Trainable params: 21,502 Non-trainable params: 0 _________________________________________________________________ . Conclusion . Parametric UMAP was similar to non-parametric and showed clusters in a truly consistent embedding. The downside is that it offers less tuning possibilities as compared to non-parametric UMAP. Most flow analysis software offers easy access to UMAP analysis but the original package offers wider potentialities that are also accessible. . UMAP is a powerful algorithm. I showed its use here in a simple flow cytometry analysis but it has numerous biological applications from flow to RNA sequencing. .",
            "url": "https://marie-annemawhin.github.io/blog/embedding/dimensionality%20reduction/flow/2021/03/07/parametric_UMAP_blog_2.html",
            "relUrl": "/embedding/dimensionality%20reduction/flow/2021/03/07/parametric_UMAP_blog_2.html",
            "date": " • Mar 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "UMAP for Flow Cytometry - Part 1",
            "content": "Flow cytometry is a powerful technique for phenotypic analysis of cells and cell populations. One main challenge in flow cytometry analysis is to visualise the resulting high-dimensional data to understand data at single-cell levels. . This is where dimensionality reduction techniques come at play, in particular with the advent of spectral and mass cytometry. Most biological data are non-linear so using algorithms based on principal component (the famous PCA) will lose local distributions even if they maintain global ones. The most used embedding algorithm (i.e. that transform high-dimension in low-dimension) is currently t-SNE, which stands for t-Stochastic Neighbour Embedding. I will tell you here why UMAP killed t-SNE as nicely said in this article. . A little bit about t-SNE before killing it . Let’s start by explaining briefly how t-SNE works: Developped by L.J.P. van der Maaten, the algorithm aims is to minimise the Kullback-Leibler (KL) divergence between the low-dimensional embedding and the high-dimensional data. The KL divergence can be defined as a measure of how one probability distribution diverges from a second. （；¬＿¬) . A quick note on KL divergence . Ok, let&#39;s plot KL divergence to better understand it using scipy.stats norm and numpy for distribution and seaborn and matplotlib.pyplot for plotting . import pandas as pd import numpy as np import matplotlib.pyplot as plt from scipy.stats import norm . Let&#39;s create a little function that plots two normal distribution and calculate the KL: . def kl_divergence(x): p = norm.pdf(x, 0, 2) q = norm.pdf(x, 3, 2) kl = p * np.log(p /q) return p, q, kl . x = np.arange(-10, 10, 0.1) p, q, kl = kl_divergence(x) . sns.set(&#39;paper&#39;, &#39;whitegrid&#39;) plt.plot(x, p, c=&#39;b&#39;) plt.plot(x, q, c=&#39;r&#39;) plt.plot(x, kl, lw=0.1, c=&#39;b&#39;) plt.fill_between(x, kl, alpha=0.2, color=&#39;b&#39;) plt.title(&#39;KL(P||Q)&#39;) plt.ylim(-0.1, 0.35) plt.show() . So the idea is to end up with: . Steps . In high dimension, t-SNE tries to determine the probability of similarity between each data points. To do so, t-SNE: | measures the (‘Euclidean’, sort of straight) distance between one point to all the other points, pairwise. | computes a similarity score using a Gaussian distribution kernel 🌰: the probability a point x would have another point as neighbour if this neighbour was picked from a normal distribution centred on the point x. | These similarity scores are then normalised to account for cluster density. The expected density around each point that determines the width of the kernel is controlled by the perplexity parameters. | To account for unbalanced density and thus disagreeing scores between two points, the similarity are symmetrised by averaging them. This gives you a matrix of similarity scores for the high-dimensional data. | . Following dimension reduction initialisation with PCA or at random, the data are embedded in low dimension. The process describe above is performed again but this time to avoid overcrowding clusters (all data points ending up in the same area) a t-distribution and not a Gaussian is used leaving heavy tails for far apart values. | The KL divergence cost function is minimised by gradient descent, i.e. by small steps.The use of KL diverge prevent t-SNE from conserving global distance in low-dimension. . | . Parameters . t-SNE has two important hyperparameters to tune: . perplexity: number of nearest neighbours whose geometric distance is preserved in embedding, i.e. the number of points that you think are in a cluster a minimum that t-SNE will be considering.Typical value range from 5-50, but the larger the dataset the larger the perplexity. . | max. number of iteration:ideally the more the better, but it is limited by computational time. A default value of 1000 is good for exploration. | . . Note: t-SNE has a cost function that is not convex, i.e. it can be stuck at a local minima and with different initialisation we can get different results. . Pros and cons . Advantages: . preserves local structure and clusters | . Disadvantages: . loss of global structure | slow computation | suffers from overcrowding in large datasets | . To go further . For simple and visual explanation, I would recommend StatQuest from Josh Starmer and this popular tutorial . UMAP . Uniform Manifold Approximation and Projection or UMAP was developed in 2018 by McInnes. UMAP has already been integrated into proprietary software for flow analysis democratising its use but its python library offers a wider range of possibility. . Steps . UMAP builts a graph of the high dimensional data | As t-SNE, UMAP relies on building a graph of the high-dimensional data, i.e. a network of nodes (point) connected by edges (line). The edge are weighted by the probability of the two points to be connected. The connection between to points is made when the radius around one point overlap the radius of another points. . How to choose the radius? UMAP chooses a radius for each points using the distance between the point and its k-th nearest neighbour. Then it computes the likelihood of connection as inversely proportional to the radius growth, making the graph &#39;fuzzy&#39;. This last point is to solve the curse of dimensionality where the distances between points become more similar with higher dimensions. . To avoid ending up with isolated points and maintaining local structure, UMAP ensures that each point is connected to a least its closest neighbour. As in t-SNE, UMAP can generate connections with incompatible weights, however instead of just averaging the weighs UMAP uses the union of the two &#39;fuzzy simplical sets&#39; (⊙_☉). . . Example of fuzzy radii and weighted connections adapted from the [page](https://pair-code.github.io/understanding-umap/) of A.Coenen and A.Pearce . UMAP optimises the low dimension | As in t-SNE, the point is to optimise the low dimension data (that do not have varying metrics like the high dimension above but Euclidean distances) so they are the closest to the ‘fuzzy’ topology define above. However, to do so instead of using KL divergence, UMAP uses cross-entropy as a loss of function. This function is not too far from KL divergence, but, by using attractive and repulsive forces, it maintains global structure better. Moreover, unlike t-SNE, UMAP does not use random initialisation for embedding but spectral embedding so UMAP is more reproducible from run to run. . Note: it is still stochastic so it does change a bit. . Finally, UMAP uses stochastic gradient descent (mini batch) instead of gradient descent so it is computed faster. . Parameters . n_neighbors: number of neighbours for the high dimensional graph. A low value will provides with local structure while a high value will focus on global structure and lose fine details. | min_dist: minimum distance between points in low dimension,i.e. how tight the points are clustered. Large values results in less pack clusters and loses focus on global structure. | . Pros and cons . Advantages: . preserves local structure and clusters | better preservation of global structure | better reproducibility | fast runtime | . Disadvantages: . hyperparameter choice is crucial and depends on the aims | . To go further . For a really playful and clear detailed explanation, checkout this website. .",
            "url": "https://marie-annemawhin.github.io/blog/embedding/dimensionality%20reduction/flow/2021/01/29/parametric_UMAP_blog.html",
            "relUrl": "/embedding/dimensionality%20reduction/flow/2021/01/29/parametric_UMAP_blog.html",
            "date": " • Jan 29, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "A little about me",
          "content": "I’m a biologist and neophyte data scientist. I have been exploring the world of machine learning and deep learning and I want to share my understanding of these models with you. I hope I can provide you with an accurate and simple insight into AI. . You can check out my CV and find out more about what I’m up to right now here. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://marie-annemawhin.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://marie-annemawhin.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}